# -*- coding: utf-8 -*-
"""BAMD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xYoWd79d_f-U0FUehM-Nmq9oAic4GhTH
"""

!pip install -U ydata-profiling[notebook]
import ydata_profiling as pp

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

# preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold

# models
from sklearn.linear_model import LinearRegression, SGDRegressor, RidgeCV
from sklearn.svm import SVR, LinearSVR
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, VotingRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.tree import DecisionTreeRegressor
import sklearn.model_selection
from sklearn.model_selection import cross_val_predict as cvp
from sklearn import metrics
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import xgboost as xgb
import lightgbm as lgbm
from sklearn.metrics import r2_score

# model tuning
from hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval

import warnings
warnings.filterwarnings("ignore")

valid_part = 0.3    #30% used for validation
pd.set_option('display.max_columns',100)  # pandas will display up to 100 columns

train0 = pd.read_csv("/content/drive/MyDrive/craigslistVehicles.csv")
train0.head(5)

DF=train0 #another file copied to data set

df=train0 #another file copied to data set

train0.columns #columns of train0

train0.info()

"""dropping columns"""

drop_columns = ['url', 'city', 'city_url', 'make', 'title_status', 'VIN', 'size', 'image_url', 'desc', 'lat','long']
train0 = train0.drop(columns = drop_columns)

"""dataset head"""

train0 = train0.dropna()
train0.head(5)

"""categorical conversion"""

numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64'] # Determination categorical features
categorical_columns = []    #categorical columns
features = train0.columns.values.tolist()
for col in features:
    if train0[col].dtype in numerics: continue        # Find categorical columns:
    categorical_columns.append(col)

for col in categorical_columns:       # Label Encoding categorical features
    if col in train0.columns:
        le = LabelEncoder()
        le.fit(list(train0[col].astype(str).values))
        train0[col] = le.transform(list(train0[col].astype(str).values))

train0['year'] = (train0['year']-1900).astype(int)    #	1900 subtracted  from the year column to reduce  numerical scale of the year feature
train0['odometer'] = train0['odometer'].astype(int)   # integer type value conversion

train0.info()

train0['price'].value_counts()              #count of values

"""Outlier reduction"""

train0 = train0[train0['price'] > 1000]
train0 = train0[train0['price'] < 40000]      #keeps only some values and does away with outliers

train0['odometer'] = train0['odometer'] // 5000       # Rounded ['odometer'] to 5000
train0 = train0[train0['year'] > 110]

train0.corr()

train0.describe()

"""Profiling report"""

pp.ProfileReport(train0)      # profiling report

target_name = 'price'
train_target0 = train0[target_name]             #settin gtrain variable
train0 = train0.drop([target_name], axis=1)

train0, test0, train_target0, test_target0 = train_test_split(train0, train_target0, test_size=0.2, random_state=0)   #train test split

train0b = train0
train_target0b = train_target0

trainb, testb, targetb, target_testb = train_test_split(train0b, train_target0b, test_size=valid_part, random_state=0)      # Synthesis valid as test for selection models

scaler = StandardScaler()       #standard scaler
train0 = pd.DataFrame(scaler.fit_transform(train0), columns = train0.columns)

train0.head(3)

train, test, target, target_test = train_test_split(train0, train_target0, test_size=valid_part, random_state=0)

test.head(3)

len(train0)

"""#setting lists for accuracy"""

acc_train_r2 = []
acc_test_r2 = []
acc_train_d = []            #setting lists for accuracy
acc_test_d = []
acc_train_rmse = []
acc_test_rmse = []

def acc_d(y_meas, y_pred):

    return mean_absolute_error(y_meas, y_pred)*len(y_meas)/sum(abs(y_meas))        # Relative error between predicted y_pred and measured y_meas values

def acc_rmse(y_meas, y_pred):

    return (mean_squared_error(y_meas, y_pred))**0.5            # RMSE between predicted y_pred and measured y_meas values

def acc_boosting_model(num,model,train,test,num_iteration=0):


    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse          # Using global lists to store accuracy metrics across multiple models


    if num_iteration > 0:
        ytrain = model.predict(train, num_iteration = num_iteration)
        ytest = model.predict(test, num_iteration = num_iteration)    # Predicting on train and test data with or without specified num_iteration
    else:
        ytrain = model.predict(train)
        ytest = model.predict(test)


    print('target = ', targetb[:5].values)
    print('ytrain = ', ytrain[:5]) # first 5 values of actual vs predicted for train dat


    acc_train_r2_num = round(r2_score(targetb, ytrain) * 100, 2)
    print('acc(r2_score) for train =', acc_train_r2_num) #store R2 score for training data
    acc_train_r2.insert(num, acc_train_r2_num)


    acc_train_d_num = round(acc_d(targetb, ytrain) * 100, 2)
    print('acc(relative error) for train =', acc_train_d_num)     # Calculate and store relative error for training data
    acc_train_d.insert(num, acc_train_d_num)


    acc_train_rmse_num = round(acc_rmse(targetb, ytrain) * 100, 2)     # Calculate and store RMSE for training data
    print('acc(rmse) for train =', acc_train_rmse_num)
    acc_train_rmse.insert(num, acc_train_rmse_num)

    # Show first 5 values of actual vs predicted for test data
    print('target_test =', target_testb[:5].values)
    print('ytest =', ytest[:5])


    acc_test_r2_num = round(r2_score(target_testb, ytest) * 100, 2)
    print('acc(r2_score) for test =', acc_test_r2_num)    # Calculate and store R2 score for test data
    acc_test_r2.insert(num, acc_test_r2_num)


    acc_test_d_num = round(acc_d(target_testb, ytest) * 100, 2)
    print('acc(relative error) for test =', acc_test_d_num)  # Calculate and store relative error for test data
    acc_test_d.insert(num, acc_test_d_num)

    acc_test_rmse_num = round(acc_rmse(target_testb, ytest) * 100, 2)
    print('acc(rmse) for test =', acc_test_rmse_num)
    acc_test_rmse.insert(num, acc_test_rmse_num)
    # Calculate and store RMSE for test data

def acc_model(num, model, train, test):
    # Calculation of accuracy of model from Sklearn by different metrics
    # global variables to store performance scores across models
    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse

    # Predict on training and test data
    ytrain = model.predict(train)
    ytest = model.predict(test)


    print('target = ', target[:5].values)    # Print first 5 true vs predicted values for training data
    print('ytrain = ', ytrain[:5])


    acc_train_r2_num = round(r2_score(target, ytrain) * 100, 2)
    print('acc(r2_score) for train =', acc_train_r2_num) # Compute and store RÂ² score for training data
    acc_train_r2.insert(num, acc_train_r2_num)


    acc_train_d_num = round(acc_d(target, ytrain) * 100, 2)
    print('acc(relative error) for train =', acc_train_d_num)# Compute and store relative error for training data
    acc_train_d.insert(num, acc_train_d_num)


    acc_train_rmse_num = round(acc_rmse(target, ytrain) * 100, 2)
    print('acc(rmse) for train =', acc_train_rmse_num)# Compute and store relative error for training data
    acc_train_rmse.insert(num, acc_train_rmse_num)


    print('target_test =', target_test[:5].values)
    print('ytest =', ytest[:5])# Compute and store relative error for training data

    acc_test_r2_num = round(r2_score(target_test, ytest) * 100, 2)    # Compute and store relative error for test data
    print('acc(r2_score) for test =', acc_test_r2_num)
    acc_test_r2.insert(num, acc_test_r2_num)


    acc_test_d_num = round(acc_d(target_test, ytest) * 100, 2)
    print('acc(relative error) for test =', acc_test_d_num)
    acc_test_d.insert(num, acc_test_d_num)    # Compute and store relative error for test data


    acc_test_rmse_num = round(acc_rmse(target_test, ytest) * 100, 2)
    print('acc(rmse) for test =', acc_test_rmse_num)    # Compute and store RMSE for test data
    acc_test_rmse.insert(num, acc_test_rmse_num)

"""MOdels for fitting and predicting data"""

# linear regression model
linreg = LinearRegression()
linreg.fit(train, target)
acc_model(0,linreg,train,test)

#support vextor model
svr = SVR()
svr.fit(train, target)
acc_model(1,svr,train,test)

linear_svr = LinearSVR()    #linear support vector model
linear_svr.fit(train, target)
acc_model(2,linear_svr,train,test)

mlp = MLPRegressor()        #mlpregression model
param_grid = {'hidden_layer_sizes': [i for i in range(2,20)],
              'activation': ['relu'],
              'solver': ['adam'],
              'learning_rate': ['constant'],
              'learning_rate_init': [0.01],
              'power_t': [0.5],
              'alpha': [0.0001],
              'max_iter': [1000],
              'early_stopping': [True],
              'warm_start': [False]}      #parameters
mlp_GS = GridSearchCV(mlp, param_grid=param_grid,
                   cv=2, verbose=True, pre_dispatch='2*n_jobs')
mlp_GS.fit(train, target)
acc_model(3,mlp_GS,train,test)

sgd = SGDRegressor()      #sgd regression model
sgd.fit(train, target)
acc_model(4,sgd,train,test)

decision_tree = DecisionTreeRegressor()
decision_tree.fit(train, target)      #decision tree model
acc_model(5,decision_tree,train,test)

random_forest = RandomForestRegressor()
random_forest.fit(train, target)
                                      #random forest model
acc_model(6,random_forest,train,test)

acc_model(6,random_forest,train,test)     #acc model

# --- XGBoost (fixed & faster) ---
import numpy as np
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

# ensure numeric dtypes
X_tr = trainb.astype(np.float32)
y_tr = targetb.astype(np.float32)
X_te = testb.astype(np.float32)

# correct estimator construction (no dict as positional arg)
xgb_clf = xgb.XGBRegressor(
    objective='reg:squarederror',
    tree_method='hist',
    eval_metric='rmse',
    n_jobs=-1,
    random_state=42
)

parameters = {
    'n_estimators': [60, 100, 120, 140],
    'learning_rate': [0.01, 0.1],
    'max_depth': [5, 7],
    'reg_lambda': [0.5],
}

# lighter CV to avoid long runtimes (use 3 folds; switch to 5 later if you want)
xgb_reg = GridSearchCV(
    estimator=xgb_clf,
    param_grid=parameters,
    cv=3,
    n_jobs=-1,
    scoring='neg_root_mean_squared_error',
    verbose=1
)

xgb_reg.fit(X_tr, y_tr)

print(f"Best CV score (neg RMSE): {xgb_reg.best_score_:.4f}")
print("Best parameters set:", xgb_reg.best_params_)

# evaluate with your helper (works with GridSearchCV since it exposes predict)
acc_boosting_model(7, xgb_reg, X_tr, X_te)

Xtrain, Xval, Ztrain, Zval = train_test_split(trainb, targetb, test_size=0.2, random_state=0)
train_set = lgbm.Dataset(Xtrain, Ztrain)    #lgbm model being buiult here
valid_set = lgbm.Dataset(Xval, Zval)

params = {
        'boosting_type':'gbdt',
        'objective': 'regression',
        'num_leaves': 31,
        'learning_rate': 0.01,
        'max_depth': -1,
        'subsample': 0.8,
        'bagging_fraction' : 1,
        'max_bin' : 5000 ,
        'bagging_freq': 20,
        'colsample_bytree': 0.6,
        'metric': 'rmse',
        'min_split_gain': 0.5,
        'min_child_weight': 1,
        'min_child_samples': 10,
        'scale_pos_weight':1,
        'zero_as_missing': False,
        'seed':0,
    }
modelL = lgbm.train(params, train_set = train_set, num_boost_round=10000,
                    valid_sets=valid_set)

"""accuracy of boosting model"""

acc_boosting_model(8,modelL,trainb,testb,modelL.best_iteration)

"""Feature importance plat"""

fig =  plt.figure(figsize = (5,5))
axes = fig.add_subplot(111)
lgbm.plot_importance(modelL,ax = axes,height = 0.5)
plt.show();
plt.close()

def hyperopt_gb_score(params):        # Define the objective function to minimize (maximize negative CV score)
    clf = GradientBoostingRegressor(**params)
    current_score = cross_val_score(clf, train, target, cv=10).mean()
    print(current_score, params)
    return current_score

space_gb = {
            'n_estimators': hp.choice('n_estimators', range(100, 1000)),
            'max_depth': hp.choice('max_depth', np.arange(2, 3, dtype=int))
        }

best = fmin(fn=hyperopt_gb_score, space=space_gb, algo=tpe.suggest, max_evals=1)
print('best:')
print(best)

params = space_eval(space_gb, best)
params

gradient_boosting = GradientBoostingRegressor(**params)
gradient_boosting.fit(train, target)
acc_model(9,gradient_boosting,train,test)     #gradient boosting

ridge = RidgeCV(cv=5)#ridge cv
ridge.fit(train, target)
acc_model(10,ridge,train,test)

bagging = BaggingRegressor()
bagging.fit(train, target)#bagging
acc_model(11,bagging,train,test)

etr = ExtraTreesRegressor()
etr.fit(train, target)
acc_model(12,etr,train,test)#extra trees regression

Ada_Boost = AdaBoostRegressor()#adaboodting
Ada_Boost.fit(train, target)
acc_model(13,Ada_Boost,train,test)

Voting_Reg = VotingRegressor(estimators=[('lin', linreg), ('ridge', ridge), ('sgd', sgd)])
Voting_Reg.fit(train, target)   #voting regressor
acc_model(14,Voting_Reg,train,test)

for lst in (acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse):
    if len(lst) > 15:         # drop the extra element at index 6 (the 2nd Random Forest insert)
        lst.pop(6)

models = pd.DataFrame({
    'Model': ['Linear Regression', 'Support Vector Machines', 'Linear SVR',
              'MLPRegressor', 'Stochastic Gradient Decent',
              'Decision Tree Regressor', 'Random Forest',  'XGB', 'LGBM',
              'GradientBoostingRegressor', 'RidgeRegressor', 'BaggingRegressor', 'ExtraTreesRegressor',
              'AdaBoostRegressor', 'VotingRegressor'],
    'r2_train': acc_train_r2,
    'r2_test': acc_test_r2,
    'd_train': acc_train_d,
    'd_test': acc_test_d,
    'rmse_train': acc_train_rmse,
    'rmse_test': acc_test_rmse
})

pd.options.display.float_format = '{:,.2f}'.format

"""## Prediction accuracy Model comparison

r squared test
"""

print('Prediction accuracy for models by R2 criterion - r2_test')
models.sort_values(by=['r2_test', 'r2_train'], ascending=False)

"""d test"""

print('Prediction accuracy for models by relative error - d_test')
models.sort_values(by=['d_test', 'd_train'], ascending=True)

"""rmse error"""

print('Prediction accuracy for models by RMSE - rmse_test')
models.sort_values(by=['rmse_test', 'rmse_train'], ascending=True)

"""# Plot of Rsquared"""

plt.figure(figsize=[25,6])
xx = models['Model']
plt.tick_params(labelsize=14)
plt.plot(xx, models['r2_train'], label = 'r2_train')
plt.plot(xx, models['r2_test'], label = 'r2_test')
plt.legend()
plt.title('R2-criterion for 15 popular models for train and test datasets')
plt.xlabel('Models')
plt.ylabel('R2-criterion, %')
plt.xticks(xx, rotation='vertical')
plt.savefig('graph.png')
plt.show()

"""# Plot of relative erroe"""

# Plot
plt.figure(figsize=[25,6])
xx = models['Model']
plt.tick_params(labelsize=14)
plt.plot(xx, models['d_train'], label = 'd_train')
plt.plot(xx, models['d_test'], label = 'd_test')
plt.legend()
plt.title('Relative errors for 15 popular models for train and test datasets')
plt.xlabel('Models')
plt.ylabel('Relative error, %')
plt.xticks(xx, rotation='vertical')
plt.savefig('graph.png')
plt.show()

"""## Plot of RMSE"""

# Plot
plt.figure(figsize=[25,6])
xx = models['Model']
plt.tick_params(labelsize=14)
plt.plot(xx, models['rmse_train'], label = 'rmse_train')
plt.plot(xx, models['rmse_test'], label = 'rmse_test')
plt.legend()
plt.title('RMSE for 15 popular models for train and test datasets')
plt.xlabel('Models')
plt.ylabel('RMSE, %')
plt.xticks(xx, rotation='vertical')
plt.savefig('graph.png')
plt.show()

"""Bar charts r squared comprison"""

# R2 Test Scores - Sorted Bar Chart
plt.figure(figsize=(12,6))
sorted_models = models.sort_values('r2_test', ascending=False)
plt.bar(sorted_models['Model'], sorted_models['r2_test'], color='skyblue')
plt.xticks(rotation=75)
plt.ylabel('RÂ² on Test (%)')
plt.title('Model Comparison by RÂ² (Test)')
plt.tight_layout()
plt.show()

"""Train v test gap(overfit check)"""

models['r2_gap'] = models['r2_train'] - models['r2_test']
plt.figure(figsize=(12,6))
plt.bar(models['Model'], models['r2_gap'], color='salmon')
plt.xticks(rotation=75)
plt.ylabel('RÂ² Gap (Train - Test)')
plt.title('Overfitting Indicator (RÂ² Gap)')
plt.axhline(0, color='black', linestyle='--')
plt.tight_layout()
plt.show()

"""heatmap of metrics"""

import seaborn as sns

metric_cols = ['r2_train', 'r2_test', 'd_train', 'd_test', 'rmse_train', 'rmse_test']
plt.figure(figsize=(12,6))
sns.heatmap(models.set_index('Model')[metric_cols], annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Model Performance Metrics Heatmap')
plt.show()

"""Residual plot

"""

from sklearn.metrics import mean_squared_error
import numpy as np

top_model = random_forest  # example
y_pred = top_model.predict(train)
residuals = target - y_pred

plt.figure(figsize=(8,6))
plt.scatter(y_pred, residuals, alpha=0.5)
plt.axhline(0, color='red', linestyle='--')
plt.xlabel('Predicted Price')
plt.ylabel('Residuals')
plt.title('Residual Plot - Random Forest')
plt.show()

""" Residual plots for top models"""

importances = pd.DataFrame({
    'feature': trainb.columns,
    'RandomForest': random_forest.feature_importances_,
    'XGB': xgb_reg.best_estimator_.feature_importances_,
    'LGBM': modelL.feature_importance()
}).set_index('feature')

importances.plot(kind='barh', figsize=(10,8))
plt.title('Feature Importance Across Models')
plt.show()

"""Top-N feature importance comparison"""

preds = pd.DataFrame({
    'Actual': target_testb,
    'RF_Pred': random_forest.predict(testb),
    'XGB_Pred': xgb_reg.predict(testb),
    'LGBM_Pred': modelL.predict(testb)
})

sns.pairplot(preds)
plt.show()

"""Pairplot of actual vs predicted

"""

preds = pd.DataFrame({
    'Actual': target_testb,
    'RF_Pred': random_forest.predict(testb),
    'XGB_Pred': xgb_reg.predict(testb),
    'LGBM_Pred': modelL.predict(testb)
})

sns.pairplot(preds)
plt.show()













train= train0

drop_columns = ['url', 'city', 'city_url', 'make', 'title_status', 'VIN', 'size', 'image_url', 'desc', 'lat','long']
train = train.drop(columns = drop_columns)

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

scaler.fit(train0)

testn = pd.DataFrame(scaler.transform(test0), columns=test0.columns)              #transforming test set

"""linear regresssion over transformed set"""

from sklearn.linear_model import LinearRegression

# Create and fit model on the SAME preprocessing as test set
model = LinearRegression()
model.fit(train0, train_target0)  # train0 and test0 should have same scaling/encoding

# Predict
preds = model.predict(testn)[:3]
print(preds)

target = train['price']     #setting target to train price i.e. y column, then deleting train['price'] column

"""Ridge regresssion over transformed set"""

target=train['price']

ridge=RidgeCV()
ridge.fit(train0, train_target0)
ridge.predict(testn)[:3]

"""installing sweetviz"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install sweetviz

import sweetviz
train.head()

"""Profiling report again"""

train_with_target = pd.concat([train0.reset_index(drop=True), train_target0.reset_index(drop=True)], axis=1)
train_with_target = train_with_target[~train_with_target.isin([np.inf, -np.inf]).any(axis=1)]
train_with_target = train_with_target.dropna()

report = pp.ProfileReport(train_with_target, title="Profiling Report")
report.to_file("profiling_report.html")

with open("profiling_report.html", "r") as f:
    html_content = f.read()

from IPython.display import display, HTML
display(HTML(html_content))

numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']
categorical_columns = []
features = train.columns.values.tolist()
for col in features:
    if train[col].dtype in numerics: continue
    categorical_columns.append(col)           # Encoding categorical features

for col in categorical_columns:
    if col in train.columns:
        le = LabelEncoder()
        le.fit(list(train[col].astype(str).values))
        train[col] = le.transform(list(train[col].astype(str).values))

def reduce_mem_usage(df, verbose=True):
    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
    start_mem = df.memory_usage().sum() / 1024**2
    for col in df.columns:
        col_type = df[col].dtypes
        if col_type in numerics:
            c_min = df[col].min()
            c_max = df[col].max()
            if str(col_type)[:3] == 'int':
                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:
                    df[col] = df[col].astype(np.int8)
                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:
                    df[col] = df[col].astype(np.int16)
                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:   #used memory usage reduction for large dataset, it was hanging like crazy
                    df[col] = df[col].astype(np.int32)
                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:
                    df[col] = df[col].astype(np.int64)
            else:
                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:
                    df[col] = df[col].astype(np.float16)
                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
                    df[col] = df[col].astype(np.float32)
                else:
                    df[col] = df[col].astype(np.float64)
    end_mem = df.memory_usage().sum() / 1024**2
    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))
    return df

train = reduce_mem_usage(train)       #reduced memory usage

import seaborn as sns
import scipy.stats as stats
from matplotlib import style
import matplotlib.gridspec as gridspec
def plotting_3_chart(df, feature):
    ## Importing seaborn, matplotlab and scipy modules.
    style.use('fivethirtyeight')

    ## Creating a customized chart. and giving in figsize and everything.
    fig = plt.figure(constrained_layout=True, figsize=(15,10))
    ## creating a grid of 3 cols and 3 rows.
    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)

    ## Customizing the histogram grid.
    ax1 = fig.add_subplot(grid[0, :2])
    ## Set the title.
    ax1.set_title('Histogram')
    ## plot the histogram.
    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)

    # customizing the QQ_plot.
    ax2 = fig.add_subplot(grid[1, :2])
    ## Set the title.
    ax2.set_title('QQ_plot')
    ## Plotting the QQ_Plot.
    stats.probplot(df.loc[:,feature], plot = ax2)

    ## Customizing the Box Plot.
    ax3 = fig.add_subplot(grid[:, 2])
    ## Set title.
    ax3.set_title('Box Plot')
    ## Plotting the box plot.
    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );

plotting_3_chart(train_new, 'price')

# Rebuild a dataframe that has both features and price for plotting
train_new = pd.DataFrame(train, columns=train0.columns)  # scaled features with column names
train_new['price'] = target.values  # target from the same split

# Now plot
plotting_3_chart(train_new, 'price')

"""price odomoter year plot"""

fig = plt.figure(figsize=(10,10))
ax = plt.axes(projection="3d")

z_points = train_new['price']
x_points = train_new['odometer']
y_points = train_new['year']
ax.scatter3D(x_points, y_points, z_points, c=z_points, cmap='hsv');

ax.set_xlabel('odometer')
ax.set_ylabel('year')
ax.set_zlabel('price')

plt.show()

"""price index plot

"""

y = np.array(train_new.price)
plt.subplot(131)
plt.plot(range(len(y)),y,'.');plt.ylabel('price');plt.xlabel('index');
plt.subplot(132)
sns.boxplot(y=train_new.price)

"""returned 5, 10,90 and 95 percentile value"""

train_stat = train.describe(percentiles = [.05,.1, .9,.95])
train_stat

train_stat.loc['max',:]-train_stat.loc['95%',:]

train_stat.loc['95%',:]-train_stat.loc['90%',:]

(train_stat.loc['max',:]-train_stat.loc['95%',:])/(train_stat.loc['95%',:]-train_stat.loc['90%',:])

train_stat.loc['10%',:]-train_stat.loc['5%',:]

train_stat.loc['5%',:]-train_stat.loc['min',:]

(train_stat.loc['5%',:]-train_stat.loc['min',:])/(train_stat.loc['10%',:]-train_stat.loc['5%',:])

train_stat.loc[['10%','90%','95%'],:]

def abnormal_filter(df, threshold_first, threshold_second):       # Abnormal values filter for DataFrame df:


    df_describe = df.describe([.05, .1, .9, .95])         # threshold_second (second diff., times)
    cols = df_describe.columns.tolist()
    i = 0
    abnorm = 0
    for col in cols:
        i += 1
        # abnormal smallest
        P10_5 = df_describe.loc['10%',col]-df_describe.loc['5%',col]
        P_max_min = df_describe.loc['max',col]-df_describe.loc['min',col]
        if P10_5 != 0:
            if (df_describe.loc['5%',col]-df_describe.loc['min',col])/P10_5 > threshold_second:
                #abnormal smallest filter
                df = df[(df[col] >= df_describe.loc['5%',col])]
                print('1: ', i, col, df_describe.loc['min',col],df_describe.loc['5%',col],df_describe.loc['10%',col])
                abnorm += 1
        else:
            if P_max_min > 0:
                if (df_describe.loc['5%',col]-df_describe.loc['min',col])/P_max_min > threshold_first:
                    # abnormal smallest filter
                    df = df[(df[col] >= df_describe.loc['5%',col])]
                    print('2: ', i, col, df_describe.loc['min',col],df_describe.loc['5%',col],df_describe.loc['max',col])
                    abnorm += 1


        # abnormal biggest
        P95_90 = df_describe.loc['95%',col]-df_describe.loc['90%',col]
        if P95_90 != 0:
            if (df_describe.loc['max',col]-df_describe.loc['95%',col])/P95_90 > threshold_second:
                #abnormal biggest filter
                df = df[(df[col] <= df_describe.loc['95%',col])]
                print('3: ', i, col, df_describe.loc['90%',col],df_describe.loc['95%',col],df_describe.loc['max',col])
                abnorm += 1
        else:
            if P_max_min > 0:
                if ((df_describe.loc['max',col]-df_describe.loc['95%',col])/P_max_min > threshold_first) & (df_describe.loc['95%',col] > 0):
                    # abnormal biggest filter
                    df = df[(df[col] <= df_describe.loc['95%',col])]
                    print('4: ', i, col, df_describe.loc['min',col],df_describe.loc['95%',col],df_describe.loc['max',col])
                    abnorm += 1
    print('Number of abnormal values =', abnorm)
    return df

train = abnormal_filter(train, 0.5, 3)
train.info()

train = train[train['price'] >= 1700]
train.info()

plotting_3_chart(train, 'price')    #plottice price chart

fig = plt.figure(figsize=(10,10))
ax = plt.axes(projection="3d")

z_points = train['price']
x_points = train['odometer']      #odoometer price and year graph
y_points = train['year']
ax.scatter3D(x_points, y_points, z_points, c=z_points, cmap='hsv');

ax.set_xlabel('odometer')
ax.set_ylabel('year')
ax.set_zlabel('price')

plt.show()

train.columns

X = train
z = target

Xtrain, Xval, Ztrain, Zval = train_test_split(X, z, test_size=0.2, random_state=0)
train_set = lgbm.Dataset(Xtrain, Ztrain)           #train and validation set
valid_set = lgbm.Dataset(Xval, Zval)

params = {
        'boosting_type':'gbdt',
        'objective': 'regression',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'max_depth': -1,
        'subsample': 0.8,
        'bagging_fraction' : 1,
        'max_bin' : 5000 ,
        'bagging_freq': 20,
        'colsample_bytree': 0.6,
        'metric': 'rmse',
        'min_split_gain': 0.5,
        'min_child_weight': 1,
        'min_child_samples': 10,
        'scale_pos_weight':1,
        'zero_as_missing': True,
        'seed':0,
    }
from lightgbm import early_stopping, log_evaluation

modelL = lgbm.train(                                #lgbm model
    params,
    train_set=train_set,
    num_boost_round=1000,
    valid_sets=[valid_set],
    callbacks=[
        early_stopping(stopping_rounds=50),
        log_evaluation(period=10)
    ]
)

r2_score(Zval, modelL.predict(Xval))      #r2 of zval and model l

fig =  plt.figure(figsize = (15,15))
axes = fig.add_subplot(111)
lgbm.plot_importance(modelL,ax = axes,height = 0.5)         #lgbm plot
plt.show();plt.close()

feature_score = pd.DataFrame(train.columns, columns = ['feature'])
feature_score['score_lgb'] = modelL.feature_importance()

data_tr  = xgb.DMatrix(Xtrain, label=Ztrain)
data_cv  = xgb.DMatrix(Xval   , label=Zval) #splitted training set to validation set
evallist = [(data_tr, 'train'), (data_cv, 'valid')]

parms = {'max_depth':8, #maximum depth of a tree
         'objective':'reg:squarederror',
         'eta'      :0.3,
         'subsample':0.8,#SGD will use this percentage of data
         'lambda':4, #L2 regularization term,>1 more conservative
         'colsample_bytree':0.9,
         'colsample_bylevel':1,
         'min_child_weight': 10}
modelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,
                  early_stopping_rounds=30, maximize=False,
                  verbose_eval=10)

print('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))

r2_score(Zval, modelx.predict(data_cv))     #r2 score of model and predict

fig =  plt.figure(figsize = (15,15))
axes = fig.add_subplot(111)         #xgb plot
xgb.plot_importance(modelx,ax = axes,height = 0.5)
plt.show();plt.close()

"""## Feature score on lbg and xgb plot"""

feature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))
feature_score     #feature scote)

from sklearn import preprocessing
train = pd.DataFrame(
    preprocessing.MinMaxScaler().fit_transform(train),
    columns=train.columns,
    index=train.index
)

"""Linear regression on feature score"""

# Linear Regression

linreg = LinearRegression()
linreg.fit(train, target)
coeff_linreg = pd.DataFrame(train.columns.delete(0))
coeff_linreg.columns = ['feature']
coeff_linreg["score_linreg"] = pd.Series(linreg.coef_)
coeff_linreg.sort_values(by='score_linreg', ascending=False)

"""linear regression on score"""

coeff_linreg["score_linreg"] = coeff_linreg["score_linreg"].abs()
feature_score = pd.merge(feature_score, coeff_linreg, on='feature')
feature_score = feature_score.fillna(0)
feature_score = feature_score.set_index('feature')
feature_score

"""# MinMax scale all importances"""

feature_score = pd.DataFrame(
    preprocessing.MinMaxScaler().fit_transform(feature_score),
    columns=feature_score.columns,
    index=feature_score.index
)

# Create mean column
feature_score['mean'] = feature_score.mean(axis=1)

# Plot the feature importances
feature_score.sort_values('mean', ascending=False).plot(kind='bar', figsize=(20, 7))

feature_score.sort_values('mean', ascending=False)

"""Feature scores"""

# Create total column with different weights
feature_score['total'] = 0.5*feature_score['score_lgb'] + 0.35*feature_score['score_xgb'] + 0.15*feature_score['score_linreg']

# Plot the feature importances
feature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 7))

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

def get_feature_importance(model, model_name, X):         #   Return a DataFrame of feature importances for models that support

    try:
        if hasattr(model, "coef_"):
            importance = model.coef_
            if hasattr(importance, "toarray"):  # for sparse matrices
                importance = importance.toarray()[0]
            importance = abs(importance)
        elif hasattr(model, "feature_importances_"):
            importance = model.feature_importances_
        elif hasattr(model, "feature_importance"):  # LightGBM booster
            importance = model.feature_importance()
        else:
            print(f" Model '{model_name}' is wrong")
            return None

        return pd.DataFrame({
            'Feature': X.columns,
            'Importance': importance,
            'Model': model_name
        })

    except Exception as e:
        print(f" model error '{model_name}': {e}")
        return None

"""all importance score models"""

all_importances = []


all_importances.append(get_feature_importance(linreg, "Linear Regression", train))      # Only call on models that support importance
all_importances.append(get_feature_importance(ridge, "Ridge Regressor", train))
all_importances.append(get_feature_importance(decision_tree, "Decision Tree", train))
all_importances.append(get_feature_importance(random_forest, "Random Forest", train))
all_importances.append(get_feature_importance(xgb_reg.best_estimator_, "XGB", train))
all_importances.append(get_feature_importance(modelL, "LGBM", train))
all_importances.append(get_feature_importance(gradient_boosting, "Gradient Boosting", train))
all_importances.append(get_feature_importance(etr, "Extra Trees", train))
all_importances.append(get_feature_importance(Ada_Boost, "AdaBoost", train))

"""Importance data for model"""

# Combine all importance data
df_importance = pd.concat([imp for imp in all_importances if imp is not None], ignore_index=True)

# Plot top 15
plt.figure(figsize=(12, 6))
top = df_importance.sort_values("Importance", ascending=False).head(15)
sns.barplot(data=top, x="Importance", y="Feature", hue="Model")
plt.title("Top 15 Feature Importances Across Models")
plt.tight_layout()
plt.show()

feature_score.sort_values('total', ascending=False)       #feature scoriung

"""Final Pricing Strategy â Based on  Evaluation Table
best model (based on low relative error d_test and strong RÂ²) is: BaggingRegressor
RÂ² test: 85.52
Relative Error (d_test): 11.47%
RMSE (test): â¹3.28 lakh
Close contenders:
VotingRegressor (RÂ² = 86.05, d_test = 12.11%)
Ridge (RÂ² = 84.89, d_test = 12.67%)
XGB (RÂ² = 85.77, d_test = 13.41%)
"""

final_model = bagging  # This is best performer
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(train0)  #full feature training set (train0)

"""testn = pd.DataFrame(scaler.transform(test0), columns=test0.columns)
predicted_prices = final_model.predict(testn)

"""

testn = pd.DataFrame(scaler.transform(test0), columns=test0.columns)
predicted_prices = final_model.predict(testn)

test0_with_price = test0.copy()
test0_with_price['Predicted_Price'] = predicted_prices.round(2)

# Rebuild train_with_price with matching rows
train_with_price = pd.DataFrame(train, columns=train0.columns)
train_with_price['price'] = target.values  # same length and index

from sklearn.preprocessing import StandardScaler
import pandas as pd
import numpy as np


scaler = StandardScaler()
scaler.fit(train0) #scaler on train0


testn = pd.DataFrame(scaler.transform(test0), columns=test0.columns)

final_model = bagging  # Predict prices using best model  from evaluation
predicted_prices = final_model.predict(testn)

#Creating output DataFrame
test0_with_price = test0.copy()
test0_with_price['Predicted_Price'] = np.round(predicted_prices, 2)


train_w = train0.copy()# Rebuilding train_with_price for reference quantiles
train_with_price['price'] = target.values


def label_price_band(price, reference_df):
    q25 = reference_df['price'].quantile(0.25)
    q75 = reference_df['price'].quantile(0.75)
    if price < q25:# Defining price bands
        return "Undervalued"
    elif price > q75:
        return "Premium"
    else:
        return "Fair"

test0_with_price['Price_Band'] = test0_with_price['Predicted_Price'].apply(
    lambda x: label_price_band(x, train_with_price)
)

test0_with_price['Selling_Price'] = (test0_with_price['Predicted_Price'] * 1.08).round(2)
#Adding business margin (e.g., 8%) to create Selling Price


test0_with_price.to_csv("final_pricing_predictions.csv", index=False)


print(" Final pricing predictions saved to 'final_pricing_predictions.csv'")
display(test0_with_price[['Predicted_Price', 'Price_Band', 'Selling_Price']].head(100))

"""

# final prediction pipeline
"""

print("Final model input features:", list(train0.columns))



"""# Location based heatmap"""

from wordcloud import WordCloud
import plotly.graph_objects as go
import plotly.express as px
import folium
from folium.plugins import HeatMap

df.sample(5)

df.isnull().sum()

"""missing data from dataset"""

import missingno as msno
msno.matrix(df)
plt.show()

df.columns

df.drop(columns=['url','image_url','VIN'],inplace=True)

"""odometer vs manufacturer plot"""

import seaborn as sns
df=df.sort_values(by=['odometer'],ascending=False)
plt.figure(figsize=(25,15))
sns.barplot(x=df.manufacturer, y=df.odometer)
plt.xticks(rotation= 90)
plt.xlabel('Manufacturer')
plt.ylabel('Odometer')
plt.show()

"""gas label plot"""

gasLabels = df[df["fuel"]=="gas"].paint_color.value_counts().head(10).index
gasValues = df[df["fuel"]=="gas"].paint_color.value_counts().head(10).values
dieselLabels = df[df["fuel"]=="diesel"].paint_color.value_counts().head(10).index
dieselValues = df[df["fuel"]=="diesel"].paint_color.value_counts().head(10).values
electricLabels = df[df["fuel"]=="electric"].paint_color.value_counts().head(10).index
electricValues = df[df["fuel"]=="electric"].paint_color.value_counts().head(10).values

from plotly.subplots import make_subplots

# Create subplots: use 'domain' type for Pie subplot
fig = make_subplots(rows=1, cols=3, specs=[[{'type':'domain'}, {'type':'domain'}, {'type':'domain'}]])
fig.add_trace(go.Pie(labels=gasLabels, values=gasValues, name="Gas Car"),
              1, 1)
fig.add_trace(go.Pie(labels=dieselLabels, values=dieselValues, name="Diesel Car"),
              1, 2)
fig.add_trace(go.Pie(labels=electricLabels, values=electricValues, name="Electric Car"),
              1, 3)

# Use `hole` to create a donut-like pie chart
fig.update_traces(hole=.4, hoverinfo="label+percent+name")

fig.show()

"""paint color plot"""

x = df.type
y = df.paint_color

fig = go.Figure(go.Histogram2d(
        x=x,
        y=y
    ))
fig.show()

"""map plot of cars"""

fig = px.scatter_mapbox(df[df["type"]=="bus"], lat="lat", lon="long", hover_name="paint_color", hover_data=["paint_color", "price"],
                        color_discrete_sequence=["fuchsia"], zoom=4, height=600)
fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

"""heat map of cars listed"""

cars=df[df["type"]=="bus"].iloc[:,17:19]
cars.rename(columns={'lat':'latitude','long':'longitude'}, inplace=True)
cars.latitude.fillna(0, inplace = True)
cars.longitude.fillna(0, inplace = True)

CarMap=folium.Map(location=[42.5,-71],zoom_start=4)
HeatMap(data=cars, radius=16).add_to(CarMap)
CarMap.save('index.html')
CarMap

df.columns
mf = df['manufacturer'].value_counts()

df['manufacturer'] = df['manufacturer'].apply(lambda s: s if str(s) in mf[:10] else 'others')

mf = df['manufacturer'].value_counts()

"""manufacturer vs listings"""

plt.plot(range(len(mf)), mf)

df.columns

md = df['make'].value_counts()

"""make vs listings"""

plt.plot(range(len(md)), md)

df['make'] = df['make'].apply(lambda s: s if str(s) in md[:50] else 'others')

"""various catergories across columns"""

for i in df.columns:
  print(i, len(df[i].value_counts().index))

p1 = df['price'].quantile(0.99)
p2 = df['price'].quantile(0.1)

"""99 and 10 percenticle price"""

print(p1, p2)

df = df[(df['price']<p1) & (df['price']>p2)]

"""99 and 5 percentile odometer"""

o1 = df['odometer'].quantile(0.99)
o2 = df['odometer'].quantile(0.05)
print(o1, o2)

df = df[(df['odometer']<o1) & (df['odometer']>o2)]

"""latitide vs plot"""

fig = plt.figure(figsize=(15,8))
sns.boxplot(data=df, x='lat', y='price')
plt.xticks(rotation=90)
plt.show()

"""make vs price"""

fig = plt.figure(figsize=(15,8))
sns.boxplot(data=df, x='make', y='price')
plt.xticks(rotation=90)
plt.show()

"""condition vs price"""

fig = plt.figure(figsize=(15,8))
sns.boxplot(data=df, x='condition', y='price')
plt.xticks(rotation=90)
plt.show()

import folium
from folium import plugins
import numpy as np

m = folium.Map([44 ,68], zoom_start=5,width="100%",height="100%")
locations = list(zip(df.dropna().lat, df.dropna().long))
icons = [folium.Icon(icon="airbnb", prefix="fa") for i in range(len(locations))]

cluster = plugins.MarkerCluster(locations=locations)
m.add_child(cluster)
m

data=df

sns.pairplot(data[['v.id', 'on road old', 'on road now', 'years', 'km', 'rating', 'condition', 'economy', 'top speed',
'hp', 'torque', 'current price']],diag_kind = 'kde')

"""## Car price predicrtion"""

df
df = df.drop(['VIN', 'url', 'image_url'], axis=1) #dropping VIN url and image url

# Keeping only the first 20crore rows
df = df.head(9000000)

df.columns

"""creating categorical anf numerical trnasformer for the pipeline"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error



target = "price"
drop_cols = ["city_url", "desc"]
X = df.drop(columns=[target] + drop_cols)
y = df[target]

numeric_features = ['year', 'odometer', 'lat', 'long']
categorical_features = [col for col in X.columns if col not in numeric_features]

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ]
)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

"""creating these 2 lists for predictions"""

results = []  # store (ModelName, R2, MAE, RMSE)
feature={}

"""linear regression"""

from sklearn.linear_model import LinearRegression


results = []       # List to store (model_name, r2, mae, rmse)
fitted_pipes = {}  # Dict to store fitted pipelines

# Define model name
name = "Linear Regression"


model = LinearRegression()# Quick train/test evaluation
pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
pipe.fit(X_train, y_train)

y_pred = pipe.predict(X_test)


r2 = r2_score(y_test, y_pred)# Metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")# Output


results.append((name, r2, mae, rmse))

#fitted pipeline
fitted_pipes[name] = pipe

feature

"""using svr"""

from sklearn.svm import SVR



name = "SVM (RBF Kernel)"
model = SVR(kernel='rbf', C=100, gamma=0.1)

#pipeline
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)

# Predictions
y_pred = pipe.predict(X_test)

# Metrics
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))# Saving results & model
fitted_pipes[name] = pipe

"""linear svr"""

from sklearn.svm import LinearSVR


name = "Linear SVR"#
model = LinearSVR(max_iter=10000)  # increased max_iter to ensure convergence

pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
# Build and fit pipeline
])
pipe.fit(X_train, y_train)

# Predictions
y_pred = pipe.predict(X_test)

# Metrics
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# Output
print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")

# Save results & fitted pipeline
results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""mlp regressor"""

from sklearn.neural_network import MLPRegressor

# Model name and definition
name = "MLPRegressor"
model = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)
#increased neurons & iterations to allow learning

# Build and fit pipeline
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)

# Predictions
y_pred = pipe.predict(X_test)

# Metrics (calculate once, reuse)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""sgd regressor"""

from sklearn.linear_model import SGDRegressor


name = "SGD Regressor"
model = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)


pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)

# Predictions
y_pred = pipe.predict(X_test)

# Metrics (calculate once, reuse)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""decision tree"""

from sklearn.tree import DecisionTreeRegressor


name = "Decision Tree"
model = DecisionTreeRegressor(random_state=42)

# Build and fit pipeline
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)

# Predictions
y_pred = pipe.predict(X_test)

# Metrics (calculate once, reuse)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""random forest"""

from sklearn.ensemble import RandomForestRegressor


name = "Random Forest"
model = RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)


pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)

# Predictions
y_pred = pipe.predict(X_test)

# Metrics (calculate once, reuse)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""xgboosting"""

import xgboost as xgb


name = "XGB"
model = xgb.XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=42, n_jobs=-1)


pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)


y_pred = pipe.predict(X_test)


r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""light gbm"""

import lightgbm as lgb


name = "LGBM"
model = lgb.LGBMRegressor(n_estimators=300, learning_rate=0.1, random_state=42, n_jobs=-1)

# Build and fit pipeline
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)

# Predictions as earlier
y_pred = pipe.predict(X_test)


r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""dradient boosting"""

from sklearn.ensemble import GradientBoostingRegressor

name = "Gradient Boosting"
model = GradientBoostingRegressor(n_estimators=300, random_state=42)


pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)


y_pred = pipe.predict(X_test)


r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""ridge regression"""

from sklearn.linear_model import Ridge

model = Ridge(alpha=1.0)
pipe = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_test)

print("Ridge â R2:", r2_score(y_test, y_pred),
      "MAE:", mean_absolute_error(y_test, y_pred),
      "RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))
results.append(("Ridge â R2:", r2_score(y_test, y_pred), mean_absolute_error(y_test, y_pred), np.sqrt(mean_squared_error(y_test, y_pred))))

"""bagging"""

from sklearn.ensemble import BaggingRegressor


name = "Bagging"
model = BaggingRegressor(n_estimators=200, random_state=42, n_jobs=-1)


pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)


y_pred = pipe.predict(X_test)


r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

from sklearn.ensemble import ExtraTreesRegressor


name = "Extra Trees"
model = ExtraTreesRegressor(n_estimators=300, random_state=42, n_jobs=-1)


pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)


y_pred = pipe.predict(X_test)

r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""adaptive boosting"""

from sklearn.ensemble import AdaBoostRegressor


name = "AdaBoost"
model = AdaBoostRegressor(n_estimators=300, random_state=42)


pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', model)
])
pipe.fit(X_train, y_train)


y_pred = pipe.predict(X_test)


r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


print(f"{name} â R2: {r2:.4f}, MAE: {mae:.2f}, RMSE: {rmse:.2f}")


results.append((name, r2, mae, rmse))
fitted_pipes[name] = pipe

"""plot of r suqared model performance"""

import matplotlib.pyplot as plt


results_df = pd.DataFrame(results, columns=["Model", "R2", "MAE", "RMSE"])
results_df = results_df.sort_values(by="R2", ascending=False)

# table
print("\n=== Model Performance Comparison ===")
print(results_df)

# Bar Chart for RÂ²
plt.figure(figsize=(10, 5))
plt.bar(results_df["Model"], results_df["R2"], color="skyblue")
plt.xticks(rotation=45, ha="right")
plt.ylabel("RÂ² Score")
plt.title("Model Performance (RÂ²)")
plt.tight_layout()
plt.show()


fig, axes = plt.subplots(1, 2, figsize=(12, 5))#  MAE and RMSE charts

axes[0].bar(results_df["Model"], results_df["MAE"], color="orange")
axes[0].set_title("Mean Absolute Error")
axes[0].tick_params(axis='x', rotation=45)
axes[0].set_ylabel("MAE")

axes[1].bar(results_df["Model"], results_df["RMSE"], color="green")
axes[1].set_title("Root Mean Squared Error")
axes[1].tick_params(axis='x', rotation=45)
axes[1].set_ylabel("RMSE")

plt.tight_layout()
plt.show()

"""voting regressor"""

from sklearn.ensemble import VotingRegressor

#top models
top_models = [
    ("RandomForest", RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1)),
    ("XGB", xgb.XGBRegressor(n_estimators=300, learning_rate=0.1, random_state=42, n_jobs=-1)),
    ("LGBM", lgb.LGBMRegressor(n_estimators=300, learning_rate=0.1, random_state=42, n_jobs=-1)),
    ("ExtraTrees", ExtraTreesRegressor(n_estimators=300, random_state=42, n_jobs=-1)),
    ("GradientBoosting", GradientBoostingRegressor(n_estimators=300, random_state=42))
]

# Assigning weights ( manually set from your RÂ² results)

weights = [0.95, 0.92, 0.91, 0.93, 0.89]

# Creating the Voting Regressor
voting_model = VotingRegressor(estimators=top_models, weights=weights)

# Pipeline with preprocessing
final_pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('model', voting_model)
])

final_pipe.fit(X_train, y_train)

y_pred = final_pipe.predict(X_test)
print("VotingRegressor â R2:", r2_score(y_test, y_pred),
      "MAE:", mean_absolute_error(y_test, y_pred),
      "RMSE:", np.sqrt(mean_squared_error(y_test, y_pred)))

def get_feature_importance(pipe):

    # feature names from preprocessing
    ohe_features = pipe.named_steps['preprocessor'].transformers_[1][1]['onehot'].get_feature_names_out(categorical_features)
    feature_names = numeric_features + list(ohe_features)

    model = pipe.named_steps['model']

    if hasattr(model, "feature_importances_"):
        return pd.Series(model.feature_importances_, index=feature_names)
    elif hasattr(model, "coef_"):
        coef = model.coef_
        if coef.ndim > 1:
            coef = coef.ravel()
        return pd.Series(coef, index=feature_names)
    else:
        return None

# Individual model feature importances
for model_name, pipe in fitted_pipes.items():
    fi = get_feature_importance(pipe)
    if fi is not None:
        print(f"\n=== Top 15 Features for {model_name} ===")
        print(fi.abs().sort_values(ascending=False).head(15))
    else:
        print(f"\n{model_name} does not gibe importances or coefficients.")

# Combined average importance
all_features = list(set().union(*[get_feature_importance(pipe).index
                                  for pipe in fitted_pipes.values()
                                  if get_feature_importance(pipe) is not None]))

feature_matrix = pd.DataFrame(index=all_features)

for model_name, pipe in fitted_pipes.items():
    fi = get_feature_importance(pipe)
    if fi is not None:
        feature_matrix[model_name] = fi.reindex(all_features).fillna(0)
    else:
        feature_matrix[model_name] = 0

feature_matrix["Average_Importance"] = feature_matrix.abs().mean(axis=1)
feature_matrix = feature_matrix.sort_values(by="Average_Importance", ascending=False)

print("\n=== Top 15 Features Across All Models ===")
print(feature_matrix.head(15))

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
import numpy as np
import pandas as pd


X_all = df.drop(columns=['price'])
y_all = df['price']

# Predicion oft prices for the entire dataset
y_pred_all = final_pipe.predict(X_all)

# Evaluating performance
r2_all = r2_score(y_all, y_pred_all)
mae_all = mean_absolute_error(y_all, y_pred_all)
rmse_all = np.sqrt(mean_squared_error(y_all, y_pred_all))

print(f"Performance on Full Dataset:")
print(f"RÂ²: {r2_all:.4f}")
print(f"MAE: {mae_all:,.2f}")
print(f"RMSE: {rmse_all:,.2f}")

#prediction vs prioce of the cards
comparison_df = df.copy()
comparison_df['Predicted_Price'] = y_pred_all
comparison_df['Error'] = comparison_df['Predicted_Price'] - comparison_df['price']


print(" Sample Predictions vs Actual")
print(comparison_df[['price', 'Predicted_Price', 'Error']].head(20))


comparison_df.to_csv("car_price_predictions_comparison.csv", index=False)
print("\nFull prediction comparison saved to car_price_predictions_comparison.csv")

"""Single car price predictor"""

def predict_price(car_features: dict):
    input_df = pd.DataFrame([car_features])
    return final_pipe.predict(input_df)[0]

#eg car, change value for change in price
example_car = {
    'city': 'newyork',
    'year': 2010,
    'manufacturer': 'toyota',
    'make': 'corolla',
    'condition': 'good',
    'cylinders': '4 cylinders',
    'fuel': 'diesel',
    'odometer': 60000,
    'title_status': 'clean',
    'transmission': 'manual',
    'drive': 'fwd',
    'size': 'mid-size',
    'type': 'sedan',
    'paint_color': 'white',
    'lat': 40.7128,
    'long': -71.0060
}

predicted_price = predict_price(example_car)
print(f"Predicted Price: ${predicted_price:,.2f}")
print()